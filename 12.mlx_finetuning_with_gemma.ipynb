{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae09e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3180582eb8df41f58ed9d4fab3301662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f\n",
    "\n",
    "from mlx_lm import generate, load\n",
    "\n",
    "model, tokenizer = load(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3058a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhy is the sky blue?<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}]\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245c885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "The sky appears blue due to a phenomenon called Rayleigh scattering. Here's a breakdown of how it works:\n",
      "\n",
      "1. **Sunlight and Colors:** Sunlight is actually made up of all the colors of the rainbow. You can see this when sunlight passes through a prism, separating into its constituent colors.\n",
      "\n",
      "2. **Entering the Atmosphere:** When sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).\n",
      "\n",
      "3. **Scattering of Light:** This collision causes the sunlight to scatter in different directions.\n",
      "\n",
      "4. **Rayleigh Scattering:**  Rayleigh scattering is the key. It states that shorter wavelengths of light (blue and violet) are scattered *much* more strongly than longer wavelengths (red and orange).  The amount of scattering is inversely proportional to the fourth power of the wavelength.  This means blue light (shorter wavelength) is scattered about 10 times more than red light.\n",
      "\n",
      "5. **Why Blue, Not Violet?** Violet light is scattered even *more* than blue light. However, there are a couple of reasons why we see a blue sky instead of a violet one:\n",
      "   * **Sunlight's Spectrum:** The sun emits slightly less violet light than blue light\n",
      "==========\n",
      "Prompt: 7 tokens, 14.883 tokens-per-sec\n",
      "Generation: 256 tokens, 41.047 tokens-per-sec\n",
      "Peak memory: 7.979 GB\n"
     ]
    }
   ],
   "source": [
    "# using sampler to control generation parameters\n",
    "from mlx_lm.sample_utils import make_sampler    \n",
    "sampler = make_sampler(temp=0.0, top_p=1.0)\n",
    "\n",
    "\n",
    "# Generating without adding a prompt template manually\n",
    "prompt = \"\"\"\n",
    "Why is the sky blue?\n",
    "\"\"\".strip()\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    sampler = sampler,\n",
    "    verbose=True,  # Set to True to see the prompt and response\n",
    "    max_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3869f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'domain_task_type', 'id', 'system_prompt_used', 'source', 'conversations'],\n",
       "        num_rows: 13528\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"teknium/trismegistus-project\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5010b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>domain_task_type</th>\n",
       "      <th>id</th>\n",
       "      <th>system_prompt_used</th>\n",
       "      <th>source</th>\n",
       "      <th>conversations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Big Man' society</td>\n",
       "      <td>Task</td>\n",
       "      <td>570a8404-3270-4aba-a47c-660359440835</td>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>DomainExpert_Occult</td>\n",
       "      <td>[{'from': 'human', 'value': 'Compose a compreh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Big Man' society</td>\n",
       "      <td>Task</td>\n",
       "      <td>ddf44765-8756-46db-a945-672050905fc0</td>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>DomainExpert_Occult</td>\n",
       "      <td>[{'from': 'human', 'value': 'Develop an intric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Big Man' society</td>\n",
       "      <td>Task</td>\n",
       "      <td>9ef38c3a-31ed-48d7-94d2-75fc588bcb2e</td>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>DomainExpert_Occult</td>\n",
       "      <td>[{'from': 'human', 'value': 'Write an extensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Big Man' society</td>\n",
       "      <td>Task</td>\n",
       "      <td>6dea7781-0f74-4692-8d1d-762c6585c280</td>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>DomainExpert_Occult</td>\n",
       "      <td>[{'from': 'human', 'value': 'Develop an intric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Black Books' of European necromancy</td>\n",
       "      <td>Task</td>\n",
       "      <td>188c6c15-d2b7-448b-b93c-505aeca2a458</td>\n",
       "      <td>You are a master of the esoteric, occult, 'Bla...</td>\n",
       "      <td>DomainExpert_Occult</td>\n",
       "      <td>[{'from': 'human', 'value': 'Devise an intrica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  topic domain_task_type  \\\n",
       "0                     'Big Man' society             Task   \n",
       "1                     'Big Man' society             Task   \n",
       "2                     'Big Man' society             Task   \n",
       "3                     'Big Man' society             Task   \n",
       "4  'Black Books' of European necromancy             Task   \n",
       "\n",
       "                                     id  \\\n",
       "0  570a8404-3270-4aba-a47c-660359440835   \n",
       "1  ddf44765-8756-46db-a945-672050905fc0   \n",
       "2  9ef38c3a-31ed-48d7-94d2-75fc588bcb2e   \n",
       "3  6dea7781-0f74-4692-8d1d-762c6585c280   \n",
       "4  188c6c15-d2b7-448b-b93c-505aeca2a458   \n",
       "\n",
       "                                  system_prompt_used               source  \\\n",
       "0  You are a master of the esoteric, occult, 'Big...  DomainExpert_Occult   \n",
       "1  You are a master of the esoteric, occult, 'Big...  DomainExpert_Occult   \n",
       "2  You are a master of the esoteric, occult, 'Big...  DomainExpert_Occult   \n",
       "3  You are a master of the esoteric, occult, 'Big...  DomainExpert_Occult   \n",
       "4  You are a master of the esoteric, occult, 'Bla...  DomainExpert_Occult   \n",
       "\n",
       "                                       conversations  \n",
       "0  [{'from': 'human', 'value': 'Compose a compreh...  \n",
       "1  [{'from': 'human', 'value': 'Develop an intric...  \n",
       "2  [{'from': 'human', 'value': 'Write an extensiv...  \n",
       "3  [{'from': 'human', 'value': 'Develop an intric...  \n",
       "4  [{'from': 'human', 'value': 'Devise an intrica...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dataset to pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9dedde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master of the esoteric, occult, 'Big Man' society, you complete tasks to the absolute best of your ability, no matter if you think you were not trained to do the task specifically, you will attempt to do it anyways, since you have performed the tasks you are given with great mastery, accuracy, and deep understanding of what is requested. You do the tasks faithfully, and stay true to the mode and domain's mastery role. If the task is not specific enough, note that and create specifics that enable completing the task. \n",
      " [{'from': 'human', 'value': \"Compose a comprehensive biography of a renowned figure in the 'Big Man' society, delving into their connections to the occult and esoteric realms. Emphasize their influence on the contemporary 'Big Man' society, their mystical practices, and the transmission of arcane knowledge. Ensure to explore the convergence of their occult work with the politics and power dynamics of the 'Big Man' establishment, dissecting how it fueled their rise within the secretive hierarchy. Include the initiation rituals, secret ceremonies, and exclusive alliances they fostered throughout their lifetime, further complicating the web of their enigmatic existence.\"}, {'from': 'gpt', 'value': \"Title: The Mystifying Odyssey of Eliphas Blackwood: Master of the 'Big Man' Society and Occult Connoisseur\\n\\nIntroduction:\\nIn the annals of the secret society known as the 'Big Man,' few names evoke as much intrigue and applaud as Eliphas Blackwood. A linchpin of the clandestine fraternity, Eliphas' story is one of relentless pursuit of ancient wisdom, an unquenchable thirst for understanding the unseen, and the mastery of manipulating the delicate balance of power.\\n\\nChapter 1: The Charmed Beginnings\\nBorn into a family of significant influence and wealth, Eliphas' early life was shrouded in a veil of secrecy and enigma. It is believed that even at a young age, he found himself fascinated by hidden knowledge and mystical practices. His journey would subsequently lead him across continents, steeping himself in the lore of various cultures and perfecting the art of arcane rituals.\\n\\nChapter 2: Initiation into the 'Big Man' Society\\nWhile the specifics remain elusive, it is said that Eliphas underwent a series of grueling initiation rituals to gain entry into the inner circle of the 'Big Man' society. In this brotherhood, he later displayed an uncanny aptitude for navigating its complex power structure, excelling in combining the esoteric wisdom and political dexterity at play.\\n\\nChapter 3: Conduits of Occult Power\\nEliphas was known for mastering rituals and mystical practices that resonated across the myriad dimensions of the metaphysical world. Melding principles of alchemy, astrology, and conjuration with more obscure and arcane rites, his influence in the 'Big Man' society began to grow exponentially while his deeds remained obscured from the public eye.\\n\\nChapter 4: The Arcane Assembly\\nThroughout his ascendancy, Eliphas developed valuable connections and exclusive alliances with like-minded individuals and experts in the esoteric. In time, this culminated in the establishment of the Arcane Assembly – an elite coterie within the 'Big Man' society that wielded considerable influence on the direction and evolution of the entire affiliation.\\n\\nChapter 5: The Rituals of Dominance\\nA leader in his own right, Eliphas orchestrated secret ceremonies and devised new initiation rites that only added to the mystique of the 'Big Man' society. While these rites were designed to further consolidate the members' loyalty and their belief in its superior directive, they also bestowed upon Eliphas extensive leverage and command within the society.\\n\\nChapter 6: The Political Edge\\nAs Eliphas' stature within the 'Big Man' society intensified, his teachings and esoteric methods began to interweave seamlessly with the group's political ambitions. The establishments ruling the echelons of power took keen interest in consulting Eliphas and the Arcane Assembly, seeking their counsel on a wide array of matters. Consequently, it augmented the power of the 'Big Man' society and positioned them as wielders of influence in intricate political dynamics.\\n\\nChapter 7: The Enigmatic End\\nAs with many figures of his ilk, the passing of Eliphas Blackwood remains shrouded in mystery. Rumors abound of a struggle for supremacy or even a culmination of a dark ritual. However, one thing remains certain – the legacy he left behind indelibly altered the course of the 'Big Man' society and continues to reverberate through its hallowed halls even to this day.\\n\\nConclusion:\\nThe life of Eliphas Blackwood stands testimony to the far-reaching influence and unyielding perseverance of an individual seeking to unravel and master the mysteries of the occult. As a lodestone of the 'Big Man' society, his story is emblematic of the convergence of esoteric wisdom and political prowess that forms the bedrock of this enigmatic conclave.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0][\"system_prompt_used\"], \"\\n\", df.iloc[0][\"conversations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e35a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_prompt_used</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>Compose a comprehensive biography of a renowne...</td>\n",
       "      <td>Title: The Mystifying Odyssey of Eliphas Black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>Develop an intricate numerology system that de...</td>\n",
       "      <td>I. Foundational Numerology\\n\\nThe 'Big Man' so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>Write an extensive biography of a prominent oc...</td>\n",
       "      <td>Title: Nathaniel Ziester: A Life in Shadows - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a master of the esoteric, occult, 'Big...</td>\n",
       "      <td>Develop an intricate system of numerology, inc...</td>\n",
       "      <td>Title: The Numerological Riddles of the Big Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a master of the esoteric, occult, 'Bla...</td>\n",
       "      <td>Devise an intricate multi-step process for the...</td>\n",
       "      <td>Step 1: Assess the condition of the grimoire\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13523</th>\n",
       "      <td>You are a master of the esoteric, occult, Reap...</td>\n",
       "      <td>In the context of the Reappropriated Goddess m...</td>\n",
       "      <td>Answer: To regain women's empowerment and infl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13524</th>\n",
       "      <td>You are a master of the esoteric, occult, Reap...</td>\n",
       "      <td>Write a section of a grimoire explaining the c...</td>\n",
       "      <td>Title: The Reappropriated Goddess in the Occul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>You are a master of the esoteric, occult, Reap...</td>\n",
       "      <td>Write a section of a grimoire specifically foc...</td>\n",
       "      <td>Title: The Reappropriated Goddess: A Journey i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>You are a master of the esoteric, occult, Reap...</td>\n",
       "      <td>Create a detailed introductory section for a g...</td>\n",
       "      <td>Title: The Reappropriated Goddess: A Grimoire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>You are a master of the esoteric, occult, Reap...</td>\n",
       "      <td>Write an introductory section of a grimoire, f...</td>\n",
       "      <td>Title: The Reappropriated Goddess: Foundations...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13528 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      system_prompt_used  \\\n",
       "0      You are a master of the esoteric, occult, 'Big...   \n",
       "1      You are a master of the esoteric, occult, 'Big...   \n",
       "2      You are a master of the esoteric, occult, 'Big...   \n",
       "3      You are a master of the esoteric, occult, 'Big...   \n",
       "4      You are a master of the esoteric, occult, 'Bla...   \n",
       "...                                                  ...   \n",
       "13523  You are a master of the esoteric, occult, Reap...   \n",
       "13524  You are a master of the esoteric, occult, Reap...   \n",
       "13525  You are a master of the esoteric, occult, Reap...   \n",
       "13526  You are a master of the esoteric, occult, Reap...   \n",
       "13527  You are a master of the esoteric, occult, Reap...   \n",
       "\n",
       "                                                question  \\\n",
       "0      Compose a comprehensive biography of a renowne...   \n",
       "1      Develop an intricate numerology system that de...   \n",
       "2      Write an extensive biography of a prominent oc...   \n",
       "3      Develop an intricate system of numerology, inc...   \n",
       "4      Devise an intricate multi-step process for the...   \n",
       "...                                                  ...   \n",
       "13523  In the context of the Reappropriated Goddess m...   \n",
       "13524  Write a section of a grimoire explaining the c...   \n",
       "13525  Write a section of a grimoire specifically foc...   \n",
       "13526  Create a detailed introductory section for a g...   \n",
       "13527  Write an introductory section of a grimoire, f...   \n",
       "\n",
       "                                                  answer  \n",
       "0      Title: The Mystifying Odyssey of Eliphas Black...  \n",
       "1      I. Foundational Numerology\\n\\nThe 'Big Man' so...  \n",
       "2      Title: Nathaniel Ziester: A Life in Shadows - ...  \n",
       "3      Title: The Numerological Riddles of the Big Ma...  \n",
       "4      Step 1: Assess the condition of the grimoire\\n...  \n",
       "...                                                  ...  \n",
       "13523  Answer: To regain women's empowerment and infl...  \n",
       "13524  Title: The Reappropriated Goddess in the Occul...  \n",
       "13525  Title: The Reappropriated Goddess: A Journey i...  \n",
       "13526  Title: The Reappropriated Goddess: A Grimoire ...  \n",
       "13527  Title: The Reappropriated Goddess: Foundations...  \n",
       "\n",
       "[13528 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the quetion and answer into separate columns\n",
    "df[[\"question\", \"answer\"]] = pd.DataFrame(df[\"conversations\"].tolist(), index=df.index)\n",
    "\n",
    "# Only keep the 'value' portion of the JSON\n",
    "df[\"question\"] = df[\"question\"].apply(lambda x: x[\"value\"])\n",
    "df[\"answer\"] = df[\"answer\"].apply(lambda x: x[\"value\"])\n",
    "\n",
    "df[[\"system_prompt_used\", \"question\", \"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58b5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are a master of the esoteric, occult, 'Big Man' society, you complete tasks to the absolute best of your ability, no matter if you think you were not trained to do the task specifically, you will attempt to do it anyways, since you have performed the tasks you are given with great mastery, accuracy, and deep understanding of what is requested. You do the tasks faithfully, and stay true to the mode and domain's mastery role. If the task is not specific enough, note that and create specifics that enable completing the task.\n",
      "## User\n",
      "Compose a comprehensive biography of a renowned figure in the 'Big Man' society, delving into their connections to the occult and esoteric realms. Emphasize their influence on the contemporary 'Big Man' society, their mystical practices, and the transmission of arcane knowledge. Ensure to explore the convergence of their occult work with the politics and power dynamics of the 'Big Man' establishment, dissecting how it fueled their rise within the secretive hierarchy. Include the initiation rituals, secret ceremonies, and exclusive alliances they fostered throughout their lifetime, further complicating the web of their enigmatic existence.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Title: The Mystifying Odyssey of Eliphas Blackwood: Master of the 'Big Man' Society and Occult Connoisseur\n",
      "\n",
      "Introduction:\n",
      "In the annals of the secret society known as the 'Big Man,' few names evoke as much intrigue and applaud as Eliphas Blackwood. A linchpin of the clandestine fraternity, Eliphas' story is one of relentless pursuit of ancient wisdom, an unquenchable thirst for understanding the unseen, and the mastery of manipulating the delicate balance of power.\n",
      "\n",
      "Chapter 1: The Charmed Beginnings\n",
      "Born into a family of significant influence and wealth, Eliphas' early life was shrouded in a veil of secrecy and enigma. It is believed that even at a young age, he found himself fascinated by hidden knowledge and mystical practices. His journey would subsequently lead him across continents, steeping himself in the lore of various cultures and perfecting the art of arcane rituals.\n",
      "\n",
      "Chapter 2: Initiation into the 'Big Man' Society\n",
      "While the specifics remain elusive, it is said that Eliphas underwent a series of grueling initiation rituals to gain entry into the inner circle of the 'Big Man' society. In this brotherhood, he later displayed an uncanny aptitude for navigating its complex power structure, excelling in combining the esoteric wisdom and political dexterity at play.\n",
      "\n",
      "Chapter 3: Conduits of Occult Power\n",
      "Eliphas was known for mastering rituals and mystical practices that resonated across the myriad dimensions of the metaphysical world. Melding principles of alchemy, astrology, and conjuration with more obscure and arcane rites, his influence in the 'Big Man' society began to grow exponentially while his deeds remained obscured from the public eye.\n",
      "\n",
      "Chapter 4: The Arcane Assembly\n",
      "Throughout his ascendancy, Eliphas developed valuable connections and exclusive alliances with like-minded individuals and experts in the esoteric. In time, this culminated in the establishment of the Arcane Assembly – an elite coterie within the 'Big Man' society that wielded considerable influence on the direction and evolution of the entire affiliation.\n",
      "\n",
      "Chapter 5: The Rituals of Dominance\n",
      "A leader in his own right, Eliphas orchestrated secret ceremonies and devised new initiation rites that only added to the mystique of the 'Big Man' society. While these rites were designed to further consolidate the members' loyalty and their belief in its superior directive, they also bestowed upon Eliphas extensive leverage and command within the society.\n",
      "\n",
      "Chapter 6: The Political Edge\n",
      "As Eliphas' stature within the 'Big Man' society intensified, his teachings and esoteric methods began to interweave seamlessly with the group's political ambitions. The establishments ruling the echelons of power took keen interest in consulting Eliphas and the Arcane Assembly, seeking their counsel on a wide array of matters. Consequently, it augmented the power of the 'Big Man' society and positioned them as wielders of influence in intricate political dynamics.\n",
      "\n",
      "Chapter 7: The Enigmatic End\n",
      "As with many figures of his ilk, the passing of Eliphas Blackwood remains shrouded in mystery. Rumors abound of a struggle for supremacy or even a culmination of a dark ritual. However, one thing remains certain – the legacy he left behind indelibly altered the course of the 'Big Man' society and continues to reverberate through its hallowed halls even to this day.\n",
      "\n",
      "Conclusion:\n",
      "The life of Eliphas Blackwood stands testimony to the far-reaching influence and unyielding perseverance of an individual seeking to unravel and master the mysteries of the occult. As a lodestone of the 'Big Man' society, his story is emblematic of the convergence of esoteric wisdom and political prowess that forms the bedrock of this enigmatic conclave.<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(row: pd.Series) -> str:\n",
    "    \"Format to Gemma's chat template\"\n",
    "    return \"\"\"<bos><start_of_turn>user\n",
    "## Instructions\n",
    "{}\n",
    "## User\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn><eos>\"\"\".format(row[\"system_prompt_used\"], row[\"question\"], row[\"answer\"])\n",
    "\n",
    "\n",
    "df[\"text\"] = df.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Let's see what the model will be trained on\n",
    "print(df[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139937ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"<bos><start_of_turn>user\\n## Instructions\\nYou are a master of the esoteric, occult, Chronotopic inversion and education, you have written many textbooks on the subject in ways that provide students with rich and deep understanding of the subject. You are being asked to write textbook-like sections on a topic and you do it with full context, explainability, and reliability in accuracy to the true facts of the topic at hand, in a textbook style that a student would easily be able to learn from, in a rich, engaging, and contextual way. Always include relevant context (such as formulas and history), related concepts, and in a way that someone can gain deep insights from.\\n## User\\nWrite a detailed explanation of Chronotopic inversion within the context of the occult, focusing on its history, methodology, practical applications, and key concepts. Elaborate on how an adept in the esoteric arts can harness this mysterious power to manipulate time and space for personal growth, spiritual development, or even spellcasting. Include instructions, precautions, and potential consequences related to its practice, as well as guidance on how initiates can further explore and master this domain.<end_of_turn>\\n<start_of_turn>model\\nChronotopic Inversion: An Esoteric Exploration of Time and Space\\n\\nI. Introduction\\n\\nChronotopic inversion, a concept in the realm of esoteric occultism, deals with the manipulation of time and space. Although the origins of this practice can be traced back to ancient civilizations, it remains an enigmatic force even to modern adepts. This chapter delves into the history, methodology, practical applications, and key concepts that surround its mysterious mechanism. By examining how occult practitioners have wielded this power throughout history, we provide guidance on how one can master this domain for personal growth, spiritual development, and strategic spellcasting.\\n\\nII. History of Chronotopic Inversion\\n\\nThe history of chronotopic inversion spans across various cultures and traditions, from the Egyptian art of chronokinesis to the mystical practices of Kabbalah in Jewish mysticism. Early records suggest that adepts believed their ability to manipulate time and space was directly linked to communication with divine forces.\\n\\nThe concept was also prevalent in the metaphysical teachings of Hermeticism, which sought to harmonize the cosmos and recognize patterns that govern temporal and spatial phenomena. Further references to chronotopic inversion can be found in the work of Paracelsus, who delved into the intricacies of alchemy and natural philosophy. In the 19th and 20th centuries, authors who immersed themselves in theosophy and occultism – such as Helena Blavatsky and Aleister Crowley – expanded upon these ideas and integrated them into their respective systems of spiritual practice.\\n\\nIII. Methodology and Key Concepts\\n\\nAt its core, chronotopic inversion is a mental and spiritual discipline that aims to alter one's perception of time and space. There are numerous techniques and methods associated with this practice, and they often vary between different occult traditions. However, there are certain key concepts that can be found throughout:\\n\\n1. Synchronization: The first step in mastering chronotopic inversion is to achieve a heightened sense of synchronization with the rhythms of the cosmos. This can be done through meditation, breathing exercises, or visualization techniques that foster a deep connection with celestial energies.\\n\\n2. Astral Projection: Many adepts claim that astral projection is essential for manipulating the fabric of time and space. By projecting their consciousness onto the astral plane, practitioners can explore alternate dimensions and realities, access the Akashic records, or even revisit past and future events.\\n\\n3. Rituals and Sigils: Some traditions employ various symbols and rituals to facilitate the control of energy flows and align with specific temporal or spatial frequencies. Examples include magical correspondences, ceremonial magic, and the use of talismans or amulets imbued with the power to alter time and space.\\n\\nIV. Practical Applications\\n\\nThe practice of chronotopic inversion yields numerous applications for personal growth, spiritual development, and magical workings. Some of these applications include:\\n\\n1. Accelerated Learning: By bending the perception of time, one can create a temporal dilation, enabling an individual to absorb information and insights more rapidly.\\n\\n2. Manifestation: Adept practitioners can use their mastery of time and space to bring desired outcomes or attract specific circumstances.\\n\\n3. Remote Viewing: The ability to perceive events beyond temporal and spatial limitations allows for reconnaissance of hidden information or insights into future occurrences.\\n\\n4. Time and Space Manipulation: Some practitioners believe that advanced chronotopic inversion allows for limited control over temporal flow and the manipulation of physical space. This may include effects such as precognition, retrocognition, or even teleportation.\\n\\nV. Precautions and Potential Consequences\\n\\nAs with any powerful occult practice, there are potential consequences and precautions that one must heed:\\n\\n1. Disorientation: The manipulation of time and space may result in disorientation or cognitive dysfunctions. Ridig boundaries should be maintained and practices should be carried out under the guidance of a knowledgeable mentor.\\n\\n2. Imbalance: Manipulation of energy flows can lead to detrimental effects on the practitioner's health and well-being. Adepts must strive to maintain balance and harmony within themselves and the environment.\\n\\n3. Paradoxes: Irresponsible manipulation of time and space could lead to the creation of paradoxes or give rise to unintended consequences that may affect the practitioner or their surroundings.\\n\\nVI. Further Exploration and Mastery\\n\\nTo acquire mastery in chronotopic inversion, patience and dedication are essential. Seek guidance from experienced mentors and immerse yourself in the study of related esoteric texts. Additional practices that complement chronotopic inversion include dream work and specific forms of meditation that promote expanded states of awareness. Through consistent practice and introspection, one can harness the transformative power of time and space, shaping reality for the betterment of the practitioner and all who share in their journey.<end_of_turn><eos>\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "split_ix = int(len(df) * 0.9)\n",
    "# shuffle data\n",
    "data = df.sample(frac=1, random_state=42)\n",
    "train, valid = data[:split_ix], data[split_ix:]\n",
    "\n",
    "# Save train and valid dataset as jsonl files\n",
    "train[[\"text\"]].to_json(\"data/train.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "valid[[\"text\"]].to_json(\"data/valid.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "!head -n 1 data/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e217b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "usage: lora.py [-h] [--model MODEL] [--train] [--data DATA]\n",
      "               [--fine-tune-type {lora,dora,full}]\n",
      "               [--optimizer {adam,adamw,sgd,adafactor}] [--mask-prompt]\n",
      "               [--num-layers NUM_LAYERS] [--batch-size BATCH_SIZE]\n",
      "               [--iters ITERS] [--val-batches VAL_BATCHES]\n",
      "               [--learning-rate LEARNING_RATE]\n",
      "               [--steps-per-report STEPS_PER_REPORT]\n",
      "               [--steps-per-eval STEPS_PER_EVAL]\n",
      "               [--resume-adapter-file RESUME_ADAPTER_FILE]\n",
      "               [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\n",
      "               [--test] [--test-batches TEST_BATCHES]\n",
      "               [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\n",
      "               [--grad-checkpoint] [--wandb WANDB] [--seed SEED]\n",
      "\n",
      "LoRA or QLoRA finetuning.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --train               Do training\n",
      "  --data DATA           Directory with {train, valid, test}.jsonl files or the\n",
      "                        name of a Hugging Face dataset (e.g., 'mlx-\n",
      "                        community/wikisql')\n",
      "  --fine-tune-type {lora,dora,full}\n",
      "                        Type of fine-tuning to perform: lora, dora, or full.\n",
      "  --optimizer {adam,adamw,sgd,adafactor}\n",
      "                        Optimizer to use for training: adam, adamw, sgd, or\n",
      "                        adafactor.\n",
      "  --mask-prompt         Mask the prompt in the loss when training\n",
      "  --num-layers NUM_LAYERS\n",
      "                        Number of layers to fine-tune. Default is 16, use -1\n",
      "                        for all.\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Minibatch size.\n",
      "  --iters ITERS         Iterations to train for.\n",
      "  --val-batches VAL_BATCHES\n",
      "                        Number of validation batches, -1 uses the entire\n",
      "                        validation set.\n",
      "  --learning-rate LEARNING_RATE\n",
      "                        Adam learning rate.\n",
      "  --steps-per-report STEPS_PER_REPORT\n",
      "                        Number of training steps between loss reporting.\n",
      "  --steps-per-eval STEPS_PER_EVAL\n",
      "                        Number of training steps between validations.\n",
      "  --resume-adapter-file RESUME_ADAPTER_FILE\n",
      "                        Load path to resume training from the given fine-tuned\n",
      "                        weights.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Save/load path for the fine-tuned weights.\n",
      "  --save-every SAVE_EVERY\n",
      "                        Save the model every N iterations.\n",
      "  --test                Evaluate on the test set after training\n",
      "  --test-batches TEST_BATCHES\n",
      "                        Number of test set batches, -1 uses the entire test\n",
      "                        set.\n",
      "  --max-seq-length MAX_SEQ_LENGTH\n",
      "                        Maximum sequence length.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        A YAML configuration file with the training options\n",
      "  --grad-checkpoint     Use gradient checkpointing to reduce memory use.\n",
      "  --wandb WANDB         WandB project name to report training metrics.\n",
      "                        Disabled if None.\n",
      "  --seed SEED           The PRNG seed\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0a760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--model MODEL] [--train] [--data DATA]\n",
      "                   [--fine-tune-type {lora,dora,full}]\n",
      "                   [--optimizer {adam,adamw,sgd,adafactor}] [--mask-prompt]\n",
      "                   [--num-layers NUM_LAYERS] [--batch-size BATCH_SIZE]\n",
      "                   [--iters ITERS] [--val-batches VAL_BATCHES]\n",
      "                   [--learning-rate LEARNING_RATE]\n",
      "                   [--steps-per-report STEPS_PER_REPORT]\n",
      "                   [--steps-per-eval STEPS_PER_EVAL]\n",
      "                   [--resume-adapter-file RESUME_ADAPTER_FILE]\n",
      "                   [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\n",
      "                   [--test] [--test-batches TEST_BATCHES]\n",
      "                   [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\n",
      "                   [--grad-checkpoint] [--wandb WANDB] [--seed SEED]\n",
      "__main__.py: error: unrecognized arguments: --lora-layers 4\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm lora \\\n",
    "    --model google/gemma-3-4b-it \\\n",
    "    --train \\\n",
    "    --iters 600 \\\n",
    "    --data data \\\n",
    "    --lora-layers 4\n",
    "    # --resume-adapter-file checkpoints/600_adapters.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9d757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'mlx_lm.generate' found in sys.modules after import of package 'mlx_lm', but prior to execution of 'mlx_lm.generate'; this may result in unpredictable behaviour\n",
      "Calling `python -m mlx_lm.generate...` directly is deprecated. Use `mlx_lm.generate...` or `python -m mlx_lm generate ...` instead.\n",
      "usage: generate.py [-h] [--model MODEL] [--adapter-path ADAPTER_PATH]\n",
      "                   [--extra-eos-token EXTRA_EOS_TOKEN [EXTRA_EOS_TOKEN ...]]\n",
      "                   [--system-prompt SYSTEM_PROMPT] [--prompt PROMPT]\n",
      "                   [--prefill-response PREFILL_RESPONSE]\n",
      "                   [--max-tokens MAX_TOKENS] [--temp TEMP] [--top-p TOP_P]\n",
      "                   [--min-p MIN_P] [--top-k TOP_K]\n",
      "                   [--xtc-probability XTC_PROBABILITY]\n",
      "                   [--xtc-threshold XTC_THRESHOLD]\n",
      "                   [--min-tokens-to-keep MIN_TOKENS_TO_KEEP] [--seed SEED]\n",
      "                   [--ignore-chat-template] [--use-default-chat-template]\n",
      "                   [--chat-template-config CHAT_TEMPLATE_CONFIG]\n",
      "                   [--verbose VERBOSE] [--max-kv-size MAX_KV_SIZE]\n",
      "                   [--prompt-cache-file PROMPT_CACHE_FILE] [--kv-bits KV_BITS]\n",
      "                   [--kv-group-size KV_GROUP_SIZE]\n",
      "                   [--quantized-kv-start QUANTIZED_KV_START]\n",
      "                   [--draft-model DRAFT_MODEL]\n",
      "                   [--num-draft-tokens NUM_DRAFT_TOKENS]\n",
      "generate.py: error: unrecognized arguments: --adapter-file checkpoints/600_adapters.npz\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate --model \"google/gemma-3-4b-it\" \\\n",
    "               --adapter-file checkpoints/600_adapters.npz \\\n",
    "               --max-tokens 256 \\\n",
    "               --prompt \"Why is the sky blue?\" \\\n",
    "               --seed 69\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a44fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master in the field of the esoteric, occult, Reappropriated Goddess and Education. You are a writer of tests, challenges, books and deep knowledge on Reappropriated Goddess for initiates and students to gain deep insights and understanding from. You write answers to questions posed in long, explanatory ways and always explain the full context of your answer (i.e., related concepts, formulas, examples, or history), as well as the step-by-step thinking process you take to answer the challenges. Be rigorous and thorough, and summarize the key themes, ideas, and conclusions at the end.\n"
     ]
    }
   ],
   "source": [
    "# I thought this system prompt was cool, so let's use this one\n",
    "system_prompt = df[\"system_prompt_used\"].unique()[-2]\n",
    "# system_prompt = \"You are a master in the field of the esoteric, occult, Reappropriated Goddess and Education. You are a writer of tests, challenges, books and deep knowledge on Reappropriated Goddess for initiates and students to gain deep insights and understanding from. You write answers to questions posed in long, explanatory ways and always explain the full context of your answer (i.e., related concepts, formulas, examples, or history), as well as the step-by-step thinking process you take to answer the challenges. Be rigorous and thorough, and summarize the key themes, ideas, and conclusions at the end.\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e789da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'mlx_lm.generate' found in sys.modules after import of package 'mlx_lm', but prior to execution of 'mlx_lm.generate'; this may result in unpredictable behaviour\n",
      "Calling `python -m mlx_lm.generate...` directly is deprecated. Use `mlx_lm.generate...` or `python -m mlx_lm generate ...` instead.\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 74693.08it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/yoonani/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/generate.py\", line 908, in <module>\n",
      "    main()\n",
      "  File \"/Users/yoonani/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/generate.py\", line 815, in main\n",
      "    model, tokenizer = load(\n",
      "                       ^^^^^\n",
      "  File \"/Users/yoonani/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/utils.py\", line 262, in load\n",
      "    model = load_adapters(model, adapter_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yoonani/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/tuner/utils.py\", line 195, in load_adapters\n",
      "    raise FileNotFoundError(f\"The adapter path does not exist: {adapter_path}\")\n",
      "FileNotFoundError: The adapter path does not exist: checkpoints/600_adapters.npz\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate --model \"google/gemma-3-4b-it\" \\\n",
    "               --adapter-path \"checkpoints/600_adapters.npz\" \\\n",
    "               --max-tokens 256 \\\n",
    "               --prompt \"Why is the sky blue?\" \\\n",
    "               --seed 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "450d5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are a master in the field of the esoteric, occult, Reappropriated Goddess and Education. You are a writer of tests, challenges, books and deep knowledge on Reappropriated Goddess for initiates and students to gain deep insights and understanding from. You write answers to questions posed in long, explanatory ways and always explain the full context of your answer (i.e., related concepts, formulas, examples, or history), as well as the step-by-step thinking process you take to answer the challenges. Be rigorous and thorough, and summarize the key themes, ideas, and conclusions at the end.\n",
      "## User\n",
      "Why is the sky blue?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Why is the sky blue?\"\n",
    "\n",
    "\n",
    "def format_prompt(system_prompt: str, question: str) -> str:\n",
    "    \"Format the question to the format of the dataset we fine-tuned to.\"\n",
    "    return \"\"\"<bos><start_of_turn>user\n",
    "## Instructions\n",
    "{}\n",
    "## User\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".format(\n",
    "        system_prompt, question\n",
    "    )\n",
    "\n",
    "\n",
    "print(format_prompt(system_prompt, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9975d394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c6786dc3448a2a98f40cb85b88c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The adapter path does not exist: adapters.npz",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the fine-tuned model with LoRA weights\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_lora, _ = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-3-4b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./adapters.npz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# adapters.npz is the final checkpoint saved at the end of training\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/utils.py:262\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy)\u001b[39m\n\u001b[32m    260\u001b[39m model, config = load_model(model_path, lazy)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     model = \u001b[43mload_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     model.eval()\n\u001b[32m    264\u001b[39m tokenizer = load_tokenizer(\n\u001b[32m    265\u001b[39m     model_path, tokenizer_config, eos_token_ids=config.get(\u001b[33m\"\u001b[39m\u001b[33meos_token_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    266\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/prac_llm/.venv/lib/python3.11/site-packages/mlx_lm/tuner/utils.py:195\u001b[39m, in \u001b[36mload_adapters\u001b[39m\u001b[34m(model, adapter_path)\u001b[39m\n\u001b[32m    193\u001b[39m adapter_path = Path(adapter_path)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_path.exists():\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe adapter path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(adapter_path / \u001b[33m\"\u001b[39m\u001b[33madapter_config.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[32m    197\u001b[39m     config = types.SimpleNamespace(**json.load(fid))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The adapter path does not exist: adapters.npz"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model with LoRA weights\n",
    "model_lora, _ = load(\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    adapter_path=\"./adapters.npz\",  # adapters.npz is the final checkpoint saved at the end of training\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac_llm (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
