{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa907b1c",
   "metadata": {},
   "source": [
    "![Attention](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4f878",
   "metadata": {},
   "source": [
    "## variants of attention mechanism\n",
    "\n",
    "![Attenion in LLM](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0cb3b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df4a9153",
   "metadata": {},
   "source": [
    "### The problem with modeling long sequences\n",
    "\n",
    "트랜스포머가 등장하기 전에는 순환 신경망(RNN)이 언어 번역을 위한 인코더-디코더 아키텍처로 가장 많이 사용되었습니다. RNN은 이전 단계의 출력이 현재 단계의 입력으로 공급되는 신경망의 한 유형으로, 텍스트와 같은 순차적 데이터에 적합합니다.\n",
    "\n",
    "인코더-디코더 RNN에서 입력 텍스트는 인코더에 입력되며, 인코더는 이를 순차적으로 처리합니다. \n",
    "- 인코더는 그림 3.4와 같이 각 단계마다 숨겨진 상태(숨겨진 계층의 내부 값, memory cell 처리: 일종의 Embedding)를 업데이트하여 최종 숨겨진 상태에서 입력 문장의 전체 의미를 포착하려고 노력합니다. \n",
    "- 그런 다음 디코더는 이 최종 숨겨진 상태를 사용하여 한 번에 한 단어씩 번역된 문장을 생성하기 시작합니다. 또한 각 단계마다 숨겨진 상태를 업데이트하여 다음 단어 예측에 필요한 문맥을 전달해야 합니다.\n",
    "- 인코더-디코더 RNN의 가장 큰 한계는 디코딩 단계에서 인코더의 이전 숨겨진 상태에 직접 액세스할 수 없다는 점입니다. \n",
    "  - 따라서 **모든 관련 정보를 캡슐화한 현재 숨겨진 상태에만 의존**합니다. \n",
    "  - 이로 인해 특히 종속성이 먼 거리에 걸쳐 있을 수 있는 복잡한 문장의 경우 문맥이 손실될 수 있습니다.\n",
    "\n",
    "![그림 3-4](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039304f",
   "metadata": {},
   "source": [
    "### Capturing data depencies with attention mechanisms\n",
    "\n",
    "![그림 3-5](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-5.png)\n",
    "\n",
    "[그림3-5] Using an attention mechanism, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the attention weights, which we will compute later. Note that this figure shows the general idea behind attention and does not depict the exact implementation of the Bahdanau mechanism, which is an RNN method outside this book’s scope.\n",
    "\n",
    "Self-Attention 은 입력 시퀀스의 각 위치가 시퀀스의 표현을 계산할 때 같은 시퀀스의 다른 모든 위치의 관련성을 고려하거나 \"주의(attend to)\"할 수 있도록 하는 메커니즘입니다. 셀프 어텐션은 GPT 시리즈와 같은 트랜스포머 아키텍처를 기반으로 하는 최신 LLM의 핵심 구성 요소입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f7743",
   "metadata": {},
   "source": [
    "![그림 3-6](https://drek4537l1klr.cloudfront.net/raschka/Figures/3-6.png)\n",
    "\n",
    "[그림 3-6] Self-attention is a mechanism in transformers used to compute more efficient input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence. In this chapter, we will code this self-attention mechanism from the ground up before we code the remaining parts of the GPT-like LLM in the following chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3650d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
